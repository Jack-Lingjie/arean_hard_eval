执行参数: ta_rejected_llama3.1_instruct_2048_default_template_v2
WARNING 09-22 09:33:12 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 09-22 09:33:12 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 09-22 09:33:12 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/ta_rejected_llama3.1_instruct_2048_default_template_v2', speculative_config=None, tokenizer='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/ta_rejected_llama3.1_instruct_2048_default_template_v2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/ta_rejected_llama3.1_instruct_2048_default_template_v2, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 09-22 09:33:18 model_runner.py:720] Starting to load model /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/ta_rejected_llama3.1_instruct_2048_default_template_v2...
INFO 09-22 09:33:45 model_runner.py:732] Loading model weights took 14.9888 GB
INFO 09-22 09:33:47 gpu_executor.py:102] # GPU blocks: 28232, # CPU blocks: 2048
INFO 09-22 09:33:50 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 09-22 09:33:50 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 09-22 09:33:59 model_runner.py:1225] Graph capturing finished in 9 secs.
model name: ta_rejected_llama3.1_instruct_2048_default_template_v2
model_path: /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/ta_rejected_llama3.1_instruct_2048_default_template_v2
use original template
Data saved to ./data/arena-ta/model_answer/ta_rejected_llama3.1_instruct_2048_default_template_v2.jsonl
Text judge ta_rejected_llama3.1_instruct_2048_default_template_v2 Begin
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='ta_rejected_llama3.1_instruct_2048_default_template_v2', baseline_model='gpt-4-1106-preview', bench_name='arena-ta')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
IMAGE judge ta_rejected_llama3.1_instruct_2048_default_template_v2 Begin
model_answers.keys() dict_keys(['Meta-Llama-3.1-8B-Instruct', 'gpt-4-1106-preview', 'ta_chosen_llama3.1_instruct_dpo_2048', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v2-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v4', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v5', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-1000', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-500', 'ta_rejected_llama3.1_instruct_2048_default_template_v2', 'ta_rejected_llama3.1_instruct_dpo_2048', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_1000', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_500', 'ta_v2_chosen_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_v2_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'tulu_uf_ta_v2_2048_default_template_dpo_1500', 'tulu_v2_8b_2048_default_template_dpo', 'tulu_v2_8b_2048_default_template_sft', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9', 'uf_llama3.1_instruct_dpo_2048_job'])
-----------------generate ta_rejected_llama3.1_instruct_2048_default_template_v2 images -----------------
images generated done.
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='ta_rejected_llama3.1_instruct_2048_default_template_v2', baseline_model='gpt-4-1106-preview')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
generate images ta_rejected_llama3.1_instruct_2048_default_template_v2
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Invalid image data.', 'param': None, 'type': None}}
Request timed out.
IMAGE judge ta_rejected_llama3.1_instruct_2048_default_template_v2 Done
运行时间: -28783293.11 分钟
----------------------------------------
执行参数: ta_chosen_llama3.1_instruct_dpo_2048_v2_1000
WARNING 09-22 10:17:19 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 09-22 10:17:19 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 09-22 10:17:19 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', speculative_config=None, tokenizer='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/ta_chosen_llama3.1_instruct_dpo_2048_v2_1000, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 09-22 10:17:25 model_runner.py:720] Starting to load model /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/ta_chosen_llama3.1_instruct_dpo_2048_v2_1000...
INFO 09-22 10:17:52 model_runner.py:732] Loading model weights took 14.9888 GB
INFO 09-22 10:17:53 gpu_executor.py:102] # GPU blocks: 28232, # CPU blocks: 2048
INFO 09-22 10:17:56 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 09-22 10:17:56 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 09-22 10:18:04 model_runner.py:1225] Graph capturing finished in 8 secs.
model name: ta_chosen_llama3.1_instruct_dpo_2048_v2_1000
model_path: /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/ta_chosen_llama3.1_instruct_dpo_2048_v2_1000
use original template
Data saved to ./data/arena-ta/model_answer/ta_chosen_llama3.1_instruct_dpo_2048_v2_1000.jsonl
Text judge ta_chosen_llama3.1_instruct_dpo_2048_v2_1000 Begin
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', baseline_model='gpt-4-1106-preview', bench_name='arena-ta')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
IMAGE judge ta_chosen_llama3.1_instruct_dpo_2048_v2_1000 Begin
model_answers.keys() dict_keys(['Meta-Llama-3.1-8B-Instruct', 'gpt-4-1106-preview', 'ta_chosen_llama3.1_instruct_dpo_2048', 'ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v2-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v4', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v5', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-1000', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-500', 'ta_rejected_llama3.1_instruct_2048_default_template_v2', 'ta_rejected_llama3.1_instruct_dpo_2048', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_1000', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_500', 'ta_v2_chosen_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_v2_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'tulu_uf_ta_v2_2048_default_template_dpo_1500', 'tulu_v2_8b_2048_default_template_dpo', 'tulu_v2_8b_2048_default_template_sft', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9', 'uf_llama3.1_instruct_dpo_2048_job'])
-----------------generate ta_chosen_llama3.1_instruct_dpo_2048_v2_1000 images -----------------
images generated done.
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', baseline_model='gpt-4-1106-preview')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
generate images ta_chosen_llama3.1_instruct_dpo_2048_v2_1000
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
IMAGE judge ta_chosen_llama3.1_instruct_dpo_2048_v2_1000 Done
运行时间: -28783337.23 分钟
----------------------------------------
执行参数: ta_chosen_llama3.1_instruct_dpo_2048_v2
WARNING 09-22 10:57:41 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 09-22 10:57:41 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 09-22 10:57:41 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/ta_chosen_llama3.1_instruct_dpo_2048_v2', speculative_config=None, tokenizer='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/ta_chosen_llama3.1_instruct_dpo_2048_v2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/ta_chosen_llama3.1_instruct_dpo_2048_v2, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 09-22 10:57:56 model_runner.py:720] Starting to load model /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/ta_chosen_llama3.1_instruct_dpo_2048_v2...
INFO 09-22 11:05:51 model_runner.py:732] Loading model weights took 14.9888 GB
INFO 09-22 11:05:51 gpu_executor.py:102] # GPU blocks: 28232, # CPU blocks: 2048
INFO 09-22 11:05:54 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 09-22 11:05:54 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 09-22 11:06:03 model_runner.py:1225] Graph capturing finished in 9 secs.
model name: ta_chosen_llama3.1_instruct_dpo_2048_v2
model_path: /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/ta_chosen_llama3.1_instruct_dpo_2048_v2
use original template
Data saved to ./data/arena-ta/model_answer/ta_chosen_llama3.1_instruct_dpo_2048_v2.jsonl
Text judge ta_chosen_llama3.1_instruct_dpo_2048_v2 Begin
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='ta_chosen_llama3.1_instruct_dpo_2048_v2', baseline_model='gpt-4-1106-preview', bench_name='arena-ta')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
IMAGE judge ta_chosen_llama3.1_instruct_dpo_2048_v2 Begin
model_answers.keys() dict_keys(['Meta-Llama-3.1-8B-Instruct', 'gpt-4-1106-preview', 'ta_chosen_llama3.1_instruct_dpo_2048', 'ta_chosen_llama3.1_instruct_dpo_2048_v2', 'ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v2-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v4', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v5', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-1000', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-500', 'ta_rejected_llama3.1_instruct_2048_default_template_v2', 'ta_rejected_llama3.1_instruct_dpo_2048', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_1000', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_500', 'ta_v2_chosen_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_v2_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'tulu_uf_ta_v2_2048_default_template_dpo_1500', 'tulu_v2_8b_2048_default_template_dpo', 'tulu_v2_8b_2048_default_template_sft', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9', 'uf_llama3.1_instruct_dpo_2048_job'])
-----------------generate ta_chosen_llama3.1_instruct_dpo_2048_v2 images -----------------
images generated done.
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='ta_chosen_llama3.1_instruct_dpo_2048_v2', baseline_model='gpt-4-1106-preview')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
generate images ta_chosen_llama3.1_instruct_dpo_2048_v2
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
IMAGE judge ta_chosen_llama3.1_instruct_dpo_2048_v2 Done
运行时间: -28783377.60 分钟
----------------------------------------
执行参数: tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15
WARNING 09-22 11:35:34 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 09-22 11:35:34 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 09-22 11:35:34 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15', speculative_config=None, tokenizer='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 09-22 11:35:41 model_runner.py:720] Starting to load model /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15...
INFO 09-22 11:36:16 model_runner.py:732] Loading model weights took 14.9888 GB
INFO 09-22 11:36:19 gpu_executor.py:102] # GPU blocks: 28232, # CPU blocks: 2048
INFO 09-22 11:36:22 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 09-22 11:36:22 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 09-22 11:36:30 model_runner.py:1225] Graph capturing finished in 9 secs.
model name: tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15
model_path: /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15
use original template
Data saved to ./data/arena-ta/model_answer/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15.jsonl
Text judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15 Begin
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15', baseline_model='gpt-4-1106-preview', bench_name='arena-ta')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
IMAGE judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15 Begin
model_answers.keys() dict_keys(['Meta-Llama-3.1-8B-Instruct', 'gpt-4-1106-preview', 'ta_chosen_llama3.1_instruct_dpo_2048', 'ta_chosen_llama3.1_instruct_dpo_2048_v2', 'ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v2-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v4', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v5', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-1000', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-500', 'ta_rejected_llama3.1_instruct_2048_default_template_v2', 'ta_rejected_llama3.1_instruct_dpo_2048', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_1000', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_500', 'ta_v2_chosen_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_v2_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'tulu_uf_ta_v2_2048_default_template_dpo_1500', 'tulu_v2_8b_2048_default_template_dpo', 'tulu_v2_8b_2048_default_template_sft', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9', 'uf_llama3.1_instruct_dpo_2048_job'])
-----------------generate tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15 images -----------------
images generated done.
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15', baseline_model='gpt-4-1106-preview')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
generate images tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15
IMAGE judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15 Done
运行时间: -28783415.50 分钟
----------------------------------------
执行参数: tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500
WARNING 09-22 12:06:04 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 09-22 12:06:04 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 09-22 12:06:04 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500', speculative_config=None, tokenizer='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 09-22 12:06:10 model_runner.py:720] Starting to load model /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500...
INFO 09-22 12:06:52 model_runner.py:732] Loading model weights took 14.9888 GB
INFO 09-22 12:06:52 gpu_executor.py:102] # GPU blocks: 28232, # CPU blocks: 2048
INFO 09-22 12:06:55 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 09-22 12:06:55 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 09-22 12:07:03 model_runner.py:1225] Graph capturing finished in 8 secs.
model name: tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500
model_path: /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500
use original template
Data saved to ./data/arena-ta/model_answer/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500.jsonl
Text judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500 Begin
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500', baseline_model='gpt-4-1106-preview', bench_name='arena-ta')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
IMAGE judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500 Begin
model_answers.keys() dict_keys(['Meta-Llama-3.1-8B-Instruct', 'gpt-4-1106-preview', 'ta_chosen_llama3.1_instruct_dpo_2048', 'ta_chosen_llama3.1_instruct_dpo_2048_v2', 'ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v2-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v4', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v5', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-1000', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-500', 'ta_rejected_llama3.1_instruct_2048_default_template_v2', 'ta_rejected_llama3.1_instruct_dpo_2048', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_1000', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_500', 'ta_v2_chosen_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_v2_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'tulu_uf_ta_v2_2048_default_template_dpo_1500', 'tulu_v2_8b_2048_default_template_dpo', 'tulu_v2_8b_2048_default_template_sft', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9', 'uf_llama3.1_instruct_dpo_2048_job'])
-----------------generate tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500 images -----------------
images generated done.
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500', baseline_model='gpt-4-1106-preview')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
generate images tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
IMAGE judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500 Done
运行时间: -28783445.98 分钟
----------------------------------------
执行参数: tulu_v2_8b_2048_default_template_sft
文件 data/arena-ta/model_answer/tulu_v2_8b_2048_default_template_sft.jsonl 存在, 跳过gen_answer.py 执行。
Text judge tulu_v2_8b_2048_default_template_sft Begin
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='tulu_v2_8b_2048_default_template_sft', baseline_model='gpt-4-1106-preview', bench_name='arena-ta')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
500 number of existing judgments
IMAGE judge tulu_v2_8b_2048_default_template_sft Begin
model_answers.keys() dict_keys(['Meta-Llama-3.1-8B-Instruct', 'gpt-4-1106-preview', 'ta_chosen_llama3.1_instruct_dpo_2048', 'ta_chosen_llama3.1_instruct_dpo_2048_v2', 'ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v2-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v4', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v5', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-1000', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-500', 'ta_rejected_llama3.1_instruct_2048_default_template_v2', 'ta_rejected_llama3.1_instruct_dpo_2048', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_1000', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_500', 'ta_v2_chosen_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_v2_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'tulu_uf_ta_v2_2048_default_template_dpo_1500', 'tulu_v2_8b_2048_default_template_dpo', 'tulu_v2_8b_2048_default_template_sft', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9', 'uf_llama3.1_instruct_dpo_2048_job'])
-----------------generate tulu_v2_8b_2048_default_template_sft images -----------------
images generated done.
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='tulu_v2_8b_2048_default_template_sft', baseline_model='gpt-4-1106-preview')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
generate images tulu_v2_8b_2048_default_template_sft
500 number of existing judgments
IMAGE judge tulu_v2_8b_2048_default_template_sft Done
运行时间: -28783475.66 分钟
----------------------------------------
执行参数: tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2
WARNING 09-22 12:36:43 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 09-22 12:36:43 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 09-22 12:36:43 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2', speculative_config=None, tokenizer='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 09-22 12:36:49 model_runner.py:720] Starting to load model /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2...
INFO 09-22 12:37:26 model_runner.py:732] Loading model weights took 14.9888 GB
INFO 09-22 12:37:29 gpu_executor.py:102] # GPU blocks: 28232, # CPU blocks: 2048
INFO 09-22 12:37:32 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 09-22 12:37:32 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 09-22 12:37:40 model_runner.py:1225] Graph capturing finished in 9 secs.
model name: tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2
model_path: /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2
use original template
Data saved to ./data/arena-ta/model_answer/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2.jsonl
Text judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2 Begin
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2', baseline_model='gpt-4-1106-preview', bench_name='arena-ta')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
IMAGE judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2 Begin
model_answers.keys() dict_keys(['Meta-Llama-3.1-8B-Instruct', 'gpt-4-1106-preview', 'ta_chosen_llama3.1_instruct_dpo_2048', 'ta_chosen_llama3.1_instruct_dpo_2048_v2', 'ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v2-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v4', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v5', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-1000', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-500', 'ta_rejected_llama3.1_instruct_2048_default_template_v2', 'ta_rejected_llama3.1_instruct_dpo_2048', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_1000', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_500', 'ta_v2_chosen_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_v2_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'tulu_uf_ta_v2_2048_default_template_dpo_1500', 'tulu_v2_8b_2048_default_template_dpo', 'tulu_v2_8b_2048_default_template_sft', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9', 'uf_llama3.1_instruct_dpo_2048_job'])
-----------------generate tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2 images -----------------
images generated done.
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2', baseline_model='gpt-4-1106-preview')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
generate images tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2
IMAGE judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2 Done
运行时间: -28783476.63 分钟
----------------------------------------
执行参数: tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2_1500
Text judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2_1500 Begin
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2_1500', baseline_model='gpt-4-1106-preview', bench_name='arena-ta')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
IMAGE judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2_1500 Begin
model_answers.keys() dict_keys(['Meta-Llama-3.1-8B-Instruct', 'gpt-4-1106-preview', 'ta_chosen_llama3.1_instruct_dpo_2048', 'ta_chosen_llama3.1_instruct_dpo_2048_v2', 'ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v2-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v4', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v5', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-1000', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-500', 'ta_rejected_llama3.1_instruct_2048_default_template_v2', 'ta_rejected_llama3.1_instruct_dpo_2048', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_1000', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_500', 'ta_v2_chosen_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_v2_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'tulu_uf_ta_v2_2048_default_template_dpo_1500', 'tulu_v2_8b_2048_default_template_dpo', 'tulu_v2_8b_2048_default_template_sft', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9', 'uf_llama3.1_instruct_dpo_2048_job'])
-----------------generate tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2_1500 images -----------------
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2_1500', baseline_model='gpt-4-1106-preview')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
generate images tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2_1500
IMAGE judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2_1500 Done
运行时间: -28783507.86 分钟
----------------------------------------
执行参数: tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14
WARNING 09-22 13:08:25 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 09-22 13:08:25 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 09-22 13:08:25 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14', speculative_config=None, tokenizer='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 09-22 13:08:31 model_runner.py:720] Starting to load model /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14...
INFO 09-22 13:09:28 model_runner.py:732] Loading model weights took 14.9888 GB
INFO 09-22 13:09:31 gpu_executor.py:102] # GPU blocks: 28232, # CPU blocks: 2048
INFO 09-22 13:09:33 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 09-22 13:09:33 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 09-22 13:09:42 model_runner.py:1225] Graph capturing finished in 8 secs.
model name: tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14
model_path: /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14
use original template
Data saved to ./data/arena-ta/model_answer/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14.jsonl
Text judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14 Begin
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14', baseline_model='gpt-4-1106-preview', bench_name='arena-ta')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
IMAGE judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14 Begin
model_answers.keys() dict_keys(['Meta-Llama-3.1-8B-Instruct', 'gpt-4-1106-preview', 'ta_chosen_llama3.1_instruct_dpo_2048', 'ta_chosen_llama3.1_instruct_dpo_2048_v2', 'ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v2-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v4', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v5', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-1000', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-500', 'ta_rejected_llama3.1_instruct_2048_default_template_v2', 'ta_rejected_llama3.1_instruct_dpo_2048', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_1000', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_500', 'ta_v2_chosen_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_v2_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'tulu_uf_ta_v2_2048_default_template_dpo_1500', 'tulu_v2_8b_2048_default_template_dpo', 'tulu_v2_8b_2048_default_template_sft', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9', 'uf_llama3.1_instruct_dpo_2048_job'])
-----------------generate tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14 images -----------------
images generated done.
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14', baseline_model='gpt-4-1106-preview')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
generate images tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14
IMAGE judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14 Done
运行时间: -28783508.33 分钟
----------------------------------------
执行参数: tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11
WARNING 09-22 13:37:12 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 09-22 13:37:12 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 09-22 13:37:12 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11', speculative_config=None, tokenizer='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 09-22 13:37:18 model_runner.py:720] Starting to load model /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11...
INFO 09-22 13:37:48 model_runner.py:732] Loading model weights took 14.9888 GB
INFO 09-22 13:37:49 gpu_executor.py:102] # GPU blocks: 28232, # CPU blocks: 2048
INFO 09-22 13:37:51 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 09-22 13:37:51 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 09-22 13:38:00 model_runner.py:1225] Graph capturing finished in 9 secs.
model name: tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11
model_path: /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11
use original template
Data saved to ./data/arena-ta/model_answer/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11.jsonl
Text judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11 Begin
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11', baseline_model='gpt-4-1106-preview', bench_name='arena-ta')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}
Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}
Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}
Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}
Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}
Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}
IMAGE judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11 Begin
model_answers.keys() dict_keys(['Meta-Llama-3.1-8B-Instruct', 'gpt-4-1106-preview', 'ta_chosen_llama3.1_instruct_dpo_2048', 'ta_chosen_llama3.1_instruct_dpo_2048_v2', 'ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v2-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v4', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v5', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-1000', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-500', 'ta_rejected_llama3.1_instruct_2048_default_template_v2', 'ta_rejected_llama3.1_instruct_dpo_2048', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_1000', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_500', 'ta_v2_chosen_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_v2_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'tulu_uf_ta_v2_2048_default_template_dpo_1500', 'tulu_v2_8b_2048_default_template_dpo', 'tulu_v2_8b_2048_default_template_sft', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9', 'uf_llama3.1_instruct_dpo_2048_job'])
-----------------generate tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11 images -----------------
images generated done.
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11', baseline_model='gpt-4-1106-preview')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
generate images tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
IMAGE judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11 Done
运行时间: -28783537.11 分钟
----------------------------------------
执行参数: tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500
WARNING 09-22 14:05:31 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 09-22 14:05:31 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 09-22 14:05:31 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500', speculative_config=None, tokenizer='/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 09-22 14:05:37 model_runner.py:720] Starting to load model /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500...
INFO 09-22 14:06:05 model_runner.py:732] Loading model weights took 14.9888 GB
INFO 09-22 14:06:05 gpu_executor.py:102] # GPU blocks: 28232, # CPU blocks: 2048
INFO 09-22 14:06:07 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 09-22 14:06:07 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 09-22 14:06:16 model_runner.py:1225] Graph capturing finished in 9 secs.
model name: tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500
model_path: /mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500
use original template
Data saved to ./data/arena-ta/model_answer/tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500.jsonl
Text judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500 Begin
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500', baseline_model='gpt-4-1106-preview', bench_name='arena-ta')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
IMAGE judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500 Begin
model_answers.keys() dict_keys(['Meta-Llama-3.1-8B-Instruct', 'gpt-4-1106-preview', 'ta_chosen_llama3.1_instruct_dpo_2048', 'ta_chosen_llama3.1_instruct_dpo_2048_v2', 'ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v2-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v4', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v5', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-1000', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-500', 'ta_rejected_llama3.1_instruct_2048_default_template_v2', 'ta_rejected_llama3.1_instruct_dpo_2048', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_1000', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_500', 'ta_v2_chosen_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_v2_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'tulu_uf_ta_v2_2048_default_template_dpo_1500', 'tulu_v2_8b_2048_default_template_dpo', 'tulu_v2_8b_2048_default_template_sft', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9', 'uf_llama3.1_instruct_dpo_2048_job'])
-----------------generate tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500 images -----------------
images generated done.
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500', baseline_model='gpt-4-1106-preview')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
generate images tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
Error code: 400 - {'error': {'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'filtered': True, 'detected': True}}}, 'code': 'content_filter', 'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: \r\nhttps://go.microsoft.com/fwlink/?linkid=2198766.", 'param': 'prompt', 'type': None}}
IMAGE judge tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500 Done
运行时间: -28783565.43 分钟
----------------------------------------
执行参数: ta_rejected_llama3.1_instruct_2048_default_template_v2
文件 data/arena-ta/model_answer/ta_rejected_llama3.1_instruct_2048_default_template_v2.jsonl 存在, 跳过gen_answer.py 执行。
Text judge ta_rejected_llama3.1_instruct_2048_default_template_v2 Begin
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='ta_rejected_llama3.1_instruct_2048_default_template_v2', baseline_model='gpt-4-1106-preview', bench_name='arena-ta')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
500 number of existing judgments
IMAGE judge ta_rejected_llama3.1_instruct_2048_default_template_v2 Begin
model_answers.keys() dict_keys(['Meta-Llama-3.1-8B-Instruct', 'gpt-4-1106-preview', 'ta_chosen_llama3.1_instruct_dpo_2048', 'ta_chosen_llama3.1_instruct_dpo_2048_v2', 'ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v2-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v4', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v5', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-1000', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-500', 'ta_rejected_llama3.1_instruct_2048_default_template_v2', 'ta_rejected_llama3.1_instruct_dpo_2048', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_1000', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_500', 'ta_v2_chosen_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_v2_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'tulu_uf_ta_v2_2048_default_template_dpo_1500', 'tulu_v2_8b_2048_default_template_dpo', 'tulu_v2_8b_2048_default_template_sft', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9', 'uf_llama3.1_instruct_dpo_2048_job'])
-----------------generate ta_rejected_llama3.1_instruct_2048_default_template_v2 images -----------------
images generated done.
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='ta_rejected_llama3.1_instruct_2048_default_template_v2', baseline_model='gpt-4-1106-preview')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
generate images ta_rejected_llama3.1_instruct_2048_default_template_v2
500 number of existing judgments
IMAGE judge ta_rejected_llama3.1_instruct_2048_default_template_v2 Done
运行时间: -28783597.10 分钟
----------------------------------------
执行参数: ta_chosen_llama3.1_instruct_dpo_2048_v2_1000
文件 data/arena-ta/model_answer/ta_chosen_llama3.1_instruct_dpo_2048_v2_1000.jsonl 存在, 跳过gen_answer.py 执行。
Text judge ta_chosen_llama3.1_instruct_dpo_2048_v2_1000 Begin
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', baseline_model='gpt-4-1106-preview', bench_name='arena-ta')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
500 number of existing judgments
IMAGE judge ta_chosen_llama3.1_instruct_dpo_2048_v2_1000 Begin
model_answers.keys() dict_keys(['Meta-Llama-3.1-8B-Instruct', 'gpt-4-1106-preview', 'ta_chosen_llama3.1_instruct_dpo_2048', 'ta_chosen_llama3.1_instruct_dpo_2048_v2', 'ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v2-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v4', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v5', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-1000', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-500', 'ta_rejected_llama3.1_instruct_2048_default_template_v2', 'ta_rejected_llama3.1_instruct_dpo_2048', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_1000', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_500', 'ta_v2_chosen_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_v2_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'tulu_uf_ta_v2_2048_default_template_dpo_1500', 'tulu_v2_8b_2048_default_template_dpo', 'tulu_v2_8b_2048_default_template_sft', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9', 'uf_llama3.1_instruct_dpo_2048_job'])
-----------------generate ta_chosen_llama3.1_instruct_dpo_2048_v2_1000 images -----------------
images generated done.
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', baseline_model='gpt-4-1106-preview')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
generate images ta_chosen_llama3.1_instruct_dpo_2048_v2_1000
500 number of existing judgments
IMAGE judge ta_chosen_llama3.1_instruct_dpo_2048_v2_1000 Done
运行时间: -28783597.85 分钟
----------------------------------------
执行参数: ta_chosen_llama3.1_instruct_dpo_2048_v2
文件 data/arena-ta/model_answer/ta_chosen_llama3.1_instruct_dpo_2048_v2.jsonl 存在, 跳过gen_answer.py 执行。
Text judge ta_chosen_llama3.1_instruct_dpo_2048_v2 Begin
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='ta_chosen_llama3.1_instruct_dpo_2048_v2', baseline_model='gpt-4-1106-preview', bench_name='arena-ta')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
500 number of existing judgments
IMAGE judge ta_chosen_llama3.1_instruct_dpo_2048_v2 Begin
model_answers.keys() dict_keys(['Meta-Llama-3.1-8B-Instruct', 'gpt-4-1106-preview', 'ta_chosen_llama3.1_instruct_dpo_2048', 'ta_chosen_llama3.1_instruct_dpo_2048_v2', 'ta_chosen_llama3.1_instruct_dpo_2048_v2_1000', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v1', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v2-1500', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v4', 'ta_llama3_instruct_dpo_list_bsz1_trible_debug_v5', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-1000', 'ta_rejected_llama3.1_instruct_2048_default_template_v2-500', 'ta_rejected_llama3.1_instruct_2048_default_template_v2', 'ta_rejected_llama3.1_instruct_dpo_2048', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_1000', 'ta_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2_500', 'ta_v2_chosen_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'ta_v2_rejected_tuluv2_dpo_2048_default_template_bsz1_acc8_v2', 'tulu_uf_ta_v2_2048_default_template_dpo_1500', 'tulu_v2_8b_2048_default_template_dpo', 'tulu_v2_8b_2048_default_template_sft', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v10_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v11_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v14', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v15_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v2', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v4', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v5_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v6_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v7_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v8_1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9-1500', 'tulu_v2_8b_default_template_dpo_list_bsz1_trible_debug_v9', 'uf_llama3.1_instruct_dpo_2048_job'])
-----------------generate ta_chosen_llama3.1_instruct_dpo_2048_v2 images -----------------
images generated done.
Namespace(setting_file='config/judge_config.yaml', endpoint_file='config/api_config.yaml', model_name='ta_chosen_llama3.1_instruct_dpo_2048_v2', baseline_model='gpt-4-1106-preview')
judge model: gpt-4o, baseline: True, baseline model: gpt-4-1106-preview, reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True
generate images ta_chosen_llama3.1_instruct_dpo_2048_v2
500 number of existing judgments
IMAGE judge ta_chosen_llama3.1_instruct_dpo_2048_v2 Done
运行时间: -28783598.58 分钟
----------------------------------------
所有任务已完成。
