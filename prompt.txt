  对于下面这个代码生成的结果存储的一条格式如下
  {
      "instruction": "What are the names of some famous actors that started their careers on Broadway?",
      "generator": "ta_v2_chosen_tuluv2_dpo_2048_default_template_bsz1_acc8_v2",
      "dataset": "helpful_base",
      "output": "Here are some famous actors who began their careers on Broadway:\n\n- **Tom Hanks**  \n- **Meryl Streep**  \n- **Julia Roberts**  \n- **Daniel Day-Lewis**  \n- **Audra McDonald**  \n- **Idina Menzel**  \n- **Hugh Jackman**  \n- **Bryan Cranston**  \n- **Angela Lansbury**  \n- **James Earl Jones**"
  },
  我需要将其转为以下jsonl的格式进行存储，其中question_id和answer_id均为index，使用dataset中的index，model_id换成model_name，content替换output，token_len使用预训练的分词器进行分词统计其长度
  {"question_id":"3f85bf93f18343dbb17e228a214a3800","answer_id":"jWNMkUBRTtfACGiD2fWDyM","model_id":"gpt-4-1106-preview","choices":[{"index":0,"turns":[{"content":"To create a sad and slow chord progression in the key of C major, you can use minor chords and certain progressions that evoke a melancholic feel. Here's a simple chord progression that you can try:\n\nC major - Am7 - Dm7 - G7\n\nTo add more depth and sadness to the progression, consider the following:\n\nCmaj7 - Am7 - Em7 - Fmaj7 - Fm6 - Cmaj7\/G - Fm6 - G7\n\nHere's how you can play these chords on the guitar or piano:\n\n**Guitar Chords:**\n```\nCmaj7: x32000\nAm7:   x02010\nEm7:   022030\nFmaj7: xx3210\nFm6:   xx0111\nCmaj7\/G: 332000\nG7:    320001\n```\n\n**Piano Chords:**\n- Cmaj7: C - E - G - B\n- Am7: A - C - E - G\n- Em7: E - G - B - D\n- Fmaj7: F - A - C - E\n- Fm6: F - Ab - C - D\n- Cmaj7\/G: G - C - E - G - B (G in the bass)\n- G7: G - B - D - F\n\nRemember to play these chords slowly and with feeling to enhance the sad emotion. You can also experiment with different inversions and voicings to see what sounds best to you.","token_len":320}]}],"tstamp":1719091586.1960244179}

  from vllm import LLM, SamplingParams
  from tqdm import tqdm
  import datasets
  import json
  import argparse 

  parser = argparse.ArgumentParser(description='Set model name.')  
  parser.add_argument('--model-name', type=str, required=True, help='Name of the model to use')  
  parser.add_argument('--model-path', type=str, default="/home/lidong1/jianglingjie/LLama-Factory/model_checkpoint/huggingface", help='Dir of the model to use') 
  args = parser.parse_args()  
  path_dir = args.model_path
  model_name = args.model_name 

  # template = "{{ '<|begin_of_text|>' }}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|start_header_id|>system<|end_header_id|>\n\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\n\n' + content + '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|eot_id|>' }}{% endif %}{% endfor %}"
  template = "{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message + '\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ 'Human: ' + content + '\nAssistant:' }}{% elif message['role'] == 'assistant' %}{{ content + '<|end_of_text|>' + '\n' }}{% endif %}{% endfor %}"

  # model_name = "Meta-Llama-3.1-8B-Instruct"
  # model_name = "Meta-Llama-3.1-8B"
  # Create an LLM.
  llm = LLM(model=f"{path_dir}/{model_name}")

  print(f"model name: {model_name}")
  print(f"model_path: {path_dir}/{model_name}")

  gen_kwargs_vllm = {
      "max_tokens": 2048,
      "top_p": 0.9,
      "top_k": 50,
      "temperature": 0.0,
      "repetition_penalty": 1.0,
  }
  tokenizer = llm.get_tokenizer()
  if tokenizer.chat_template is None:
      tokenizer.chat_template = template
      tokenizer.chat_template = tokenizer.chat_template.replace("<|eot_id|>", tokenizer.eos_token)
      # tokenizer.chat_template
      gen_kwargs_vllm['stop_token_ids'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids("<|eot_id|>")]
      print(f"tokenizer.chat_template: {tokenizer.chat_template}")
      print("tokenizer is None, use setted template")
  else:
      gen_kwargs_vllm['stop_token_ids'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids("<|end_of_text|>")]
      print("use original template")
  # messages = tokenizer.apply_chat_template(messages, tokenize=False)


  sampling_params = SamplingParams(**gen_kwargs_vllm)

  eval_set = datasets.load_dataset("tatsu-lab/alpaca_eval", "alpaca_eval")["eval"]

  def convert_to_message(example):  
      messages = [{"role": "user", "content": example["instruction"]}]  
      example["messages"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  
      return example  
  eval_set = eval_set.map(convert_to_message)
  # eval_set['messages']
  outputs = llm.generate(eval_set['messages'], sampling_params)
  outputs_text = [x.outputs[0].text for x in outputs]
  eval_set = eval_set.remove_columns(["output"])  # Remove the existing 'output' column if it exists  
  eval_set = eval_set.remove_columns(["messages"])
  eval_set = eval_set.add_column("output", outputs_text)  
  def rename_generator(sample):
      sample['generator'] = f"{model_name}"
      return sample
  eval_set = eval_set.map(rename_generator)
  eval_set.to_json(f"{model_name}.jsonl", batch_size=128, num_proc=8)

  ## save data

  export_dataset = eval_set
  export_data_list = [dict(row) for row in export_dataset]
  print(f"export data length {len(export_data_list)}")
  with open(f'./data/{model_name}.json', 'w', encoding='utf-8') as f:  
      json.dump(export_data_list, f, ensure_ascii=False, indent=4)  