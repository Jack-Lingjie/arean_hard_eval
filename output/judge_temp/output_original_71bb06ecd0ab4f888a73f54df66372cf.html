<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang xml:lang>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>output_original_71bb06ecd0ab4f888a73f54df66372cf</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <style type="text/css">


article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
nav,
section,
summary {
display: block;
}

audio,
canvas,
video {
display: inline-block;
}

audio:not([controls]) {
display: none;
height: 0;
}

[hidden],
template {
display: none;
}


html {
font-family: sans-serif; 
-ms-text-size-adjust: 100%; 
-webkit-text-size-adjust: 100%; 
}

body {
margin: 0;
}


a {
background: transparent;
}

a:focus {
outline: thin dotted;
}

a:active,
a:hover {
outline: 0;
}


h1 {
font-size: 2em;
margin: 0.67em 0;
}

abbr[title] {
border-bottom: 1px dotted;
}

b,
strong {
font-weight: bold;
}

dfn {
font-style: italic;
}

hr {
-moz-box-sizing: content-box;
box-sizing: content-box;
height: 0;
}

mark {
background: #ff0;
color: #000;
}

code,
kbd,
pre,
samp {
font-family: monospace, serif;
font-size: 1em;
}

pre {
white-space: pre-wrap;
}

q {
quotes: "\201C" "\201D" "\2018" "\2019";
}

small {
font-size: 80%;
}

sub,
sup {
font-size: 75%;
line-height: 0;
position: relative;
vertical-align: baseline;
}
sup {
top: -0.5em;
}
sub {
bottom: -0.25em;
}


img {
border: 0;
}

svg:not(:root) {
overflow: hidden;
}


figure {
margin: 0;
}


fieldset {
border: 1px solid #c0c0c0;
margin: 0 2px;
padding: 0.35em 0.625em 0.75em;
}

legend {
border: 0; 
padding: 0; 
}

button,
input,
select,
textarea {
font-family: inherit; 
font-size: 100%; 
margin: 0; 
}

button,
input {
line-height: normal;
}

button,
select {
text-transform: none;
}

button,
html input[type="button"], 
input[type="reset"],
input[type="submit"] {
-webkit-appearance: button; 
cursor: pointer; 
}

button[disabled],
html input[disabled] {
cursor: default;
}

input[type="checkbox"],
input[type="radio"] {
box-sizing: border-box; 
padding: 0; 
}

input[type="search"] {
-webkit-appearance: textfield; 
-moz-box-sizing: content-box;
-webkit-box-sizing: content-box; 
box-sizing: content-box;
}

input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
-webkit-appearance: none;
}

button::-moz-focus-inner,
input::-moz-focus-inner {
border: 0;
padding: 0;
}

textarea {
overflow: auto; 
vertical-align: top; 
}


table {
border-collapse: collapse;
border-spacing: 0;
}
.go-top {
position: fixed;
bottom: 2em;
right: 2em;
text-decoration: none;
background-color: #E0E0E0;
font-size: 12px;
padding: 1em;
display: inline;
}

html,body{ margin: auto;
padding-right: 1em;
padding-left: 1em;
max-width: 44em; color:black;}*:not('#mkdbuttons'){margin:0;padding:0}body{font:13.34px helvetica,arial,freesans,clean,sans-serif;-webkit-font-smoothing:subpixel-antialiased;line-height:1.4;padding:3px;background:#fff;border-radius:3px;-moz-border-radius:3px;-webkit-border-radius:3px}p{margin:1em 0}a{color:#4183c4;text-decoration:none}body{background-color:#fff;padding:30px;margin:15px;font-size:14px;line-height:1.6}body>*:first-child{margin-top:0!important}body>*:last-child{margin-bottom:0!important}@media screen{body{box-shadow:0 0 0 1px #cacaca,0 0 0 4px #eee}}h1,h2,h3,h4,h5,h6{margin:20px 0 10px;padding:0;font-weight:bold;-webkit-font-smoothing:subpixel-antialiased;cursor:text}h1{font-size:28px;color:#000}h2{font-size:24px;border-bottom:1px solid #ccc;color:#000}h3{font-size:18px;color:#333}h4{font-size:16px;color:#333}h5{font-size:14px;color:#333}h6{color:#777;font-size:14px}p,blockquote,table,pre{margin:15px 0}ul{padding-left:30px}ol{padding-left:30px}ol li ul:first-of-type{margin-top:0}hr{background:transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;border:0 none;color:#ccc;height:4px;padding:0}body>h2:first-child{margin-top:0;padding-top:0}body>h1:first-child{margin-top:0;padding-top:0}body>h1:first-child+h2{margin-top:0;padding-top:0}body>h3:first-child,body>h4:first-child,body>h5:first-child,body>h6:first-child{margin-top:0;padding-top:0}a:first-child h1,a:first-child h2,a:first-child h3,a:first-child h4,a:first-child h5,a:first-child h6{margin-top:0;padding-top:0}h1+p,h2+p,h3+p,h4+p,h5+p,h6+p,ul li>:first-child,ol li>:first-child{margin-top:0}dl{padding:0}dl dt{font-size:14px;font-weight:bold;font-style:italic;padding:0;margin:15px 0 5px}dl dt:first-child{padding:0}dl dt>:first-child{margin-top:0}dl dt>:last-child{margin-bottom:0}dl dd{margin:0 0 15px;padding:0 15px}dl dd>:first-child{margin-top:0}dl dd>:last-child{margin-bottom:0}blockquote{border-left:4px solid #DDD;padding:0 15px;color:#777}blockquote>:first-child{margin-top:0}blockquote>:last-child{margin-bottom:0}table{border-collapse:collapse;border-spacing:0;font-size:100%;font:inherit}table th{font-weight:bold;border:1px solid #ccc;padding:6px 13px}table td{border:1px solid #ccc;padding:6px 13px}table tr{border-top:1px solid #ccc;background-color:#fff}table tr:nth-child(2n){background-color:#f8f8f8}img{max-width:100%}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px;font-family:Consolas,'Liberation Mono',Courier,monospace;font-size:12px;color:#333}pre>code{margin:0;padding:0;white-space:pre;border:0;background:transparent}.highlight pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}pre code,pre tt{background-color:transparent;border:0}.poetry pre{font-family:Georgia,Garamond,serif!important;font-style:italic;font-size:110%!important;line-height:1.6em;display:block;margin-left:1em}.poetry pre code{font-family:Georgia,Garamond,serif!important;word-break:break-all;word-break:break-word;-webkit-hyphens:auto;-moz-hyphens:auto;hyphens:auto;white-space:pre-wrap}sup,sub,a.footnote{font-size:1.4ex;height:0;line-height:1;vertical-align:super;position:relative}sub{vertical-align:sub;top:-1px}@media print{body{background:#fff}img,pre,blockquote,table,figure{page-break-inside:avoid}body{background:#fff;border:0}code{background-color:#fff;color:#333!important;padding:0 .2em;border:1px solid #dedede}pre{background:#fff}pre code{background-color:white!important;overflow:visible}}@media screen{body.inverted{color:#eee!important;border-color:#555;box-shadow:none}.inverted body,.inverted hr .inverted p,.inverted td,.inverted li,.inverted h1,.inverted h2,.inverted h3,.inverted h4,.inverted h5,.inverted h6,.inverted th,.inverted .math,.inverted caption,.inverted dd,.inverted dt,.inverted blockquote{color:#eee!important;border-color:#555;box-shadow:none}.inverted td,.inverted th{background:#333}.inverted h2{border-color:#555}.inverted hr{border-color:#777;border-width:1px!important}::selection{background:rgba(157,193,200,0.5)}h1::selection{background-color:rgba(45,156,208,0.3)}h2::selection{background-color:rgba(90,182,224,0.3)}h3::selection,h4::selection,h5::selection,h6::selection,li::selection,ol::selection{background-color:rgba(133,201,232,0.3)}code::selection{background-color:rgba(0,0,0,0.7);color:#eee}code span::selection{background-color:rgba(0,0,0,0.7)!important;color:#eee!important}a::selection{background-color:rgba(255,230,102,0.2)}.inverted a::selection{background-color:rgba(255,230,102,0.6)}td::selection,th::selection,caption::selection{background-color:rgba(180,237,95,0.5)}.inverted{background:#0b2531;background:#252a2a}.inverted body{background:#252a2a}.inverted a{color:#acd1d5}}.highlight .c{color:#998;font-style:italic}.highlight .err{color:#a61717;background-color:#e3d2d2}.highlight .k,.highlight .o{font-weight:bold}.highlight .cm{color:#998;font-style:italic}.highlight .cp{color:#999;font-weight:bold}.highlight .c1{color:#998;font-style:italic}.highlight .cs{color:#999;font-weight:bold;font-style:italic}.highlight .gd{color:#000;background-color:#fdd}.highlight .gd .x{color:#000;background-color:#faa}.highlight .ge{font-style:italic}.highlight .gr{color:#a00}.highlight .gh{color:#999}.highlight .gi{color:#000;background-color:#dfd}.highlight .gi .x{color:#000;background-color:#afa}.highlight .go{color:#888}.highlight .gp{color:#555}.highlight .gs{font-weight:bold}.highlight .gu{color:#800080;font-weight:bold}.highlight .gt{color:#a00}.highlight .kc,.highlight .kd,.highlight .kn,.highlight .kp,.highlight .kr{font-weight:bold}.highlight .kt{color:#458;font-weight:bold}.highlight .m{color:#099}.highlight .s{color:#d14}.highlight .na{color:#008080}.highlight .nb{color:#0086b3}.highlight .nc{color:#458;font-weight:bold}.highlight .no{color:#008080}.highlight .ni{color:#800080}.highlight .ne,.highlight .nf{color:#900;font-weight:bold}.highlight .nn{color:#555}.highlight .nt{color:#000080}.highlight .nv{color:#008080}.highlight .ow{font-weight:bold}.highlight .w{color:#bbb}.highlight .mf,.highlight .mh,.highlight .mi,.highlight .mo{color:#099}.highlight .sb,.highlight .sc,.highlight .sd,.highlight .s2,.highlight .se,.highlight .sh,.highlight .si,.highlight .sx{color:#d14}.highlight .sr{color:#009926}.highlight .s1{color:#d14}.highlight .ss{color:#990073}.highlight .bp{color:#999}.highlight .vc,.highlight .vg,.highlight .vi{color:#008080}.highlight .il{color:#099}.highlight .gc{color:#999;background-color:#eaf2f5}.type-csharp .highlight .k,.type-csharp .highlight .kt{color:#00F}.type-csharp .highlight .nf{color:#000;font-weight:normal}.type-csharp .highlight .nc{color:#2b91af}.type-csharp .highlight .nn{color:#000}.type-csharp .highlight .s,.type-csharp .highlight .sc{color:#a31515}
</style>
</head>
<body>
<p>To train a Vision Transformer (ViT) deep learning model in TensorFlow, you can follow these steps:</p>
<ol type="1">
<li><p><strong>Install Required Libraries</strong>: Ensure you have TensorFlow installed. You can install it using pip:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb1-1" title="1"><span class="ex">pip</span> install tensorflow</a></code></pre></div></li>
<li><p><strong>Import Necessary Libraries</strong>:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1"><span class="im">import</span> tensorflow <span class="im">as</span> tf</a>
<a class="sourceLine" id="cb2-2" title="2"><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers, Model</a></code></pre></div></li>
<li><p><strong>Define the Vision Transformer Architecture</strong>: Here is a simplified version of the Vision Transformer model:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1"><span class="kw">class</span> MultiHeadSelfAttention(layers.Layer):</a>
<a class="sourceLine" id="cb3-2" title="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim, num_heads<span class="op">=</span><span class="dv">8</span>):</a>
<a class="sourceLine" id="cb3-3" title="3">        <span class="bu">super</span>(MultiHeadSelfAttention, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb3-4" title="4">        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</a>
<a class="sourceLine" id="cb3-5" title="5">        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</a>
<a class="sourceLine" id="cb3-6" title="6">        <span class="va">self</span>.projection_dim <span class="op">=</span> embed_dim <span class="op">//</span> num_heads</a>
<a class="sourceLine" id="cb3-7" title="7">        <span class="va">self</span>.query_dense <span class="op">=</span> layers.Dense(embed_dim)</a>
<a class="sourceLine" id="cb3-8" title="8">        <span class="va">self</span>.key_dense <span class="op">=</span> layers.Dense(embed_dim)</a>
<a class="sourceLine" id="cb3-9" title="9">        <span class="va">self</span>.value_dense <span class="op">=</span> layers.Dense(embed_dim)</a>
<a class="sourceLine" id="cb3-10" title="10">        <span class="va">self</span>.combine_heads <span class="op">=</span> layers.Dense(embed_dim)</a>
<a class="sourceLine" id="cb3-11" title="11"></a>
<a class="sourceLine" id="cb3-12" title="12">    <span class="kw">def</span> attention(<span class="va">self</span>, query, key, value):</a>
<a class="sourceLine" id="cb3-13" title="13">        score <span class="op">=</span> tf.matmul(query, key, transpose_b<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb3-14" title="14">        dim_key <span class="op">=</span> tf.cast(tf.shape(key)[<span class="op">-</span><span class="dv">1</span>], tf.float32)</a>
<a class="sourceLine" id="cb3-15" title="15">        scaled_score <span class="op">=</span> score <span class="op">/</span> tf.math.sqrt(dim_key)</a>
<a class="sourceLine" id="cb3-16" title="16">        weights <span class="op">=</span> tf.nn.softmax(scaled_score, axis<span class="op">=-</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb3-17" title="17">        output <span class="op">=</span> tf.matmul(weights, value)</a>
<a class="sourceLine" id="cb3-18" title="18">        <span class="cf">return</span> output, weights</a>
<a class="sourceLine" id="cb3-19" title="19"></a>
<a class="sourceLine" id="cb3-20" title="20">    <span class="kw">def</span> separate_heads(<span class="va">self</span>, x, batch_size):</a>
<a class="sourceLine" id="cb3-21" title="21">        x <span class="op">=</span> tf.reshape(x, (batch_size, <span class="dv">-1</span>, <span class="va">self</span>.num_heads, <span class="va">self</span>.projection_dim))</a>
<a class="sourceLine" id="cb3-22" title="22">        <span class="cf">return</span> tf.transpose(x, perm<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>])</a>
<a class="sourceLine" id="cb3-23" title="23"></a>
<a class="sourceLine" id="cb3-24" title="24">    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</a>
<a class="sourceLine" id="cb3-25" title="25">        batch_size <span class="op">=</span> tf.shape(inputs)[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb3-26" title="26">        query <span class="op">=</span> <span class="va">self</span>.query_dense(inputs)</a>
<a class="sourceLine" id="cb3-27" title="27">        key <span class="op">=</span> <span class="va">self</span>.key_dense(inputs)</a>
<a class="sourceLine" id="cb3-28" title="28">        value <span class="op">=</span> <span class="va">self</span>.value_dense(inputs)</a>
<a class="sourceLine" id="cb3-29" title="29">        query <span class="op">=</span> <span class="va">self</span>.separate_heads(query, batch_size)</a>
<a class="sourceLine" id="cb3-30" title="30">        key <span class="op">=</span> <span class="va">self</span>.separate_heads(key, batch_size)</a>
<a class="sourceLine" id="cb3-31" title="31">        value <span class="op">=</span> <span class="va">self</span>.separate_heads(value, batch_size)</a>
<a class="sourceLine" id="cb3-32" title="32">        attention, weights <span class="op">=</span> <span class="va">self</span>.attention(query, key, value)</a>
<a class="sourceLine" id="cb3-33" title="33">        attention <span class="op">=</span> tf.transpose(attention, perm<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>])</a>
<a class="sourceLine" id="cb3-34" title="34">        concat_attention <span class="op">=</span> tf.reshape(attention, (batch_size, <span class="dv">-1</span>, <span class="va">self</span>.embed_dim))</a>
<a class="sourceLine" id="cb3-35" title="35">        output <span class="op">=</span> <span class="va">self</span>.combine_heads(concat_attention)</a>
<a class="sourceLine" id="cb3-36" title="36">        <span class="cf">return</span> output</a>
<a class="sourceLine" id="cb3-37" title="37"></a>
<a class="sourceLine" id="cb3-38" title="38"><span class="kw">class</span> TransformerBlock(layers.Layer):</a>
<a class="sourceLine" id="cb3-39" title="39">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim, num_heads, mlp_dim, dropout<span class="op">=</span><span class="fl">0.1</span>):</a>
<a class="sourceLine" id="cb3-40" title="40">        <span class="bu">super</span>(TransformerBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb3-41" title="41">        <span class="va">self</span>.att <span class="op">=</span> MultiHeadSelfAttention(embed_dim, num_heads)</a>
<a class="sourceLine" id="cb3-42" title="42">        <span class="va">self</span>.mlp <span class="op">=</span> tf.keras.Sequential(</a>
<a class="sourceLine" id="cb3-43" title="43">            [</a>
<a class="sourceLine" id="cb3-44" title="44">                layers.Dense(mlp_dim, activation<span class="op">=</span>tf.keras.activations.gelu),</a>
<a class="sourceLine" id="cb3-45" title="45">                layers.Dense(embed_dim),</a>
<a class="sourceLine" id="cb3-46" title="46">            ]</a>
<a class="sourceLine" id="cb3-47" title="47">        )</a>
<a class="sourceLine" id="cb3-48" title="48">        <span class="va">self</span>.layernorm1 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-6</span>)</a>
<a class="sourceLine" id="cb3-49" title="49">        <span class="va">self</span>.layernorm2 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-6</span>)</a>
<a class="sourceLine" id="cb3-50" title="50">        <span class="va">self</span>.dropout1 <span class="op">=</span> layers.Dropout(dropout)</a>
<a class="sourceLine" id="cb3-51" title="51">        <span class="va">self</span>.dropout2 <span class="op">=</span> layers.Dropout(dropout)</a>
<a class="sourceLine" id="cb3-52" title="52"></a>
<a class="sourceLine" id="cb3-53" title="53">    <span class="kw">def</span> call(<span class="va">self</span>, inputs, training):</a>
<a class="sourceLine" id="cb3-54" title="54">        attn_output <span class="op">=</span> <span class="va">self</span>.att(inputs)</a>
<a class="sourceLine" id="cb3-55" title="55">        attn_output <span class="op">=</span> <span class="va">self</span>.dropout1(attn_output, training<span class="op">=</span>training)</a>
<a class="sourceLine" id="cb3-56" title="56">        out1 <span class="op">=</span> <span class="va">self</span>.layernorm1(inputs <span class="op">+</span> attn_output)</a>
<a class="sourceLine" id="cb3-57" title="57">        mlp_output <span class="op">=</span> <span class="va">self</span>.mlp(out1)</a>
<a class="sourceLine" id="cb3-58" title="58">        mlp_output <span class="op">=</span> <span class="va">self</span>.dropout2(mlp_output, training<span class="op">=</span>training)</a>
<a class="sourceLine" id="cb3-59" title="59">        <span class="cf">return</span> <span class="va">self</span>.layernorm2(out1 <span class="op">+</span> mlp_output)</a>
<a class="sourceLine" id="cb3-60" title="60"></a>
<a class="sourceLine" id="cb3-61" title="61"><span class="kw">class</span> VisionTransformer(Model):</a>
<a class="sourceLine" id="cb3-62" title="62">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, image_size, patch_size, num_layers, num_classes, d_model, num_heads, mlp_dim, channels<span class="op">=</span><span class="dv">3</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</a>
<a class="sourceLine" id="cb3-63" title="63">        <span class="bu">super</span>(VisionTransformer, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb3-64" title="64">        num_patches <span class="op">=</span> (image_size <span class="op">//</span> patch_size) <span class="op">**</span> <span class="dv">2</span></a>
<a class="sourceLine" id="cb3-65" title="65">        <span class="va">self</span>.patch_dim <span class="op">=</span> channels <span class="op">*</span> patch_size <span class="op">**</span> <span class="dv">2</span></a>
<a class="sourceLine" id="cb3-66" title="66"></a>
<a class="sourceLine" id="cb3-67" title="67">        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</a>
<a class="sourceLine" id="cb3-68" title="68">        <span class="va">self</span>.d_model <span class="op">=</span> d_model</a>
<a class="sourceLine" id="cb3-69" title="69">        <span class="va">self</span>.num_layers <span class="op">=</span> num_layers</a>
<a class="sourceLine" id="cb3-70" title="70"></a>
<a class="sourceLine" id="cb3-71" title="71">        <span class="va">self</span>.pos_emb <span class="op">=</span> <span class="va">self</span>.add_weight(<span class="st">&quot;pos_emb&quot;</span>, shape<span class="op">=</span>(<span class="dv">1</span>, num_patches <span class="op">+</span> <span class="dv">1</span>, d_model))</a>
<a class="sourceLine" id="cb3-72" title="72">        <span class="va">self</span>.class_emb <span class="op">=</span> <span class="va">self</span>.add_weight(<span class="st">&quot;class_emb&quot;</span>, shape<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>, d_model))</a>
<a class="sourceLine" id="cb3-73" title="73">        <span class="va">self</span>.patch_proj <span class="op">=</span> layers.Dense(d_model)</a>
<a class="sourceLine" id="cb3-74" title="74">        <span class="va">self</span>.enc_layers <span class="op">=</span> [</a>
<a class="sourceLine" id="cb3-75" title="75">            TransformerBlock(d_model, num_heads, mlp_dim, dropout)</a>
<a class="sourceLine" id="cb3-76" title="76">            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</a>
<a class="sourceLine" id="cb3-77" title="77">        ]</a>
<a class="sourceLine" id="cb3-78" title="78">        <span class="va">self</span>.mlp_head <span class="op">=</span> tf.keras.Sequential(</a>
<a class="sourceLine" id="cb3-79" title="79">            [</a>
<a class="sourceLine" id="cb3-80" title="80">                layers.Dense(mlp_dim, activation<span class="op">=</span>tf.keras.activations.gelu),</a>
<a class="sourceLine" id="cb3-81" title="81">                layers.Dropout(dropout),</a>
<a class="sourceLine" id="cb3-82" title="82">                layers.Dense(num_classes),</a>
<a class="sourceLine" id="cb3-83" title="83">            ]</a>
<a class="sourceLine" id="cb3-84" title="84">        )</a>
<a class="sourceLine" id="cb3-85" title="85"></a>
<a class="sourceLine" id="cb3-86" title="86">    <span class="kw">def</span> extract_patches(<span class="va">self</span>, images):</a>
<a class="sourceLine" id="cb3-87" title="87">        batch_size <span class="op">=</span> tf.shape(images)[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb3-88" title="88">        patches <span class="op">=</span> tf.image.extract_patches(</a>
<a class="sourceLine" id="cb3-89" title="89">            images<span class="op">=</span>images,</a>
<a class="sourceLine" id="cb3-90" title="90">            sizes<span class="op">=</span>[<span class="dv">1</span>, <span class="va">self</span>.patch_size, <span class="va">self</span>.patch_size, <span class="dv">1</span>],</a>
<a class="sourceLine" id="cb3-91" title="91">            strides<span class="op">=</span>[<span class="dv">1</span>, <span class="va">self</span>.patch_size, <span class="va">self</span>.patch_size, <span class="dv">1</span>],</a>
<a class="sourceLine" id="cb3-92" title="92">            rates<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>],</a>
<a class="sourceLine" id="cb3-93" title="93">            padding<span class="op">=</span><span class="st">&quot;VALID&quot;</span>,</a>
<a class="sourceLine" id="cb3-94" title="94">        )</a>
<a class="sourceLine" id="cb3-95" title="95">        patches <span class="op">=</span> tf.reshape(patches, [batch_size, <span class="dv">-1</span>, <span class="va">self</span>.patch_dim])</a>
<a class="sourceLine" id="cb3-96" title="96">        <span class="cf">return</span> patches</a>
<a class="sourceLine" id="cb3-97" title="97"></a>
<a class="sourceLine" id="cb3-98" title="98">    <span class="kw">def</span> call(<span class="va">self</span>, inputs, training):</a>
<a class="sourceLine" id="cb3-99" title="99">        batch_size <span class="op">=</span> tf.shape(inputs)[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb3-100" title="100">        x <span class="op">=</span> <span class="va">self</span>.extract_patches(inputs)</a>
<a class="sourceLine" id="cb3-101" title="101">        x <span class="op">=</span> <span class="va">self</span>.patch_proj(x)</a>
<a class="sourceLine" id="cb3-102" title="102">        class_emb <span class="op">=</span> tf.broadcast_to(</a>
<a class="sourceLine" id="cb3-103" title="103">            <span class="va">self</span>.class_emb, [batch_size, <span class="dv">1</span>, <span class="va">self</span>.d_model]</a>
<a class="sourceLine" id="cb3-104" title="104">        )</a>
<a class="sourceLine" id="cb3-105" title="105">        x <span class="op">=</span> tf.concat([class_emb, x], axis<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb3-106" title="106">        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pos_emb</a>
<a class="sourceLine" id="cb3-107" title="107"></a>
<a class="sourceLine" id="cb3-108" title="108">        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.enc_layers:</a>
<a class="sourceLine" id="cb3-109" title="109">            x <span class="op">=</span> layer(x, training)</a>
<a class="sourceLine" id="cb3-110" title="110"></a>
<a class="sourceLine" id="cb3-111" title="111">        x <span class="op">=</span> tf.reduce_mean(x, axis<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb3-112" title="112">        <span class="cf">return</span> <span class="va">self</span>.mlp_head(x)</a></code></pre></div></li>
<li><p><strong>Prepare Your Dataset</strong>: Load and preprocess your dataset. Ensure the images are resized to the desired <code>image_size</code> and normalized.</p></li>
<li><p><strong>Create and Compile the Model</strong>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1">image_size <span class="op">=</span> <span class="dv">224</span></a>
<a class="sourceLine" id="cb4-2" title="2">patch_size <span class="op">=</span> <span class="dv">16</span></a>
<a class="sourceLine" id="cb4-3" title="3">num_layers <span class="op">=</span> <span class="dv">12</span></a>
<a class="sourceLine" id="cb4-4" title="4">num_classes <span class="op">=</span> <span class="dv">1000</span></a>
<a class="sourceLine" id="cb4-5" title="5">d_model <span class="op">=</span> <span class="dv">768</span></a>
<a class="sourceLine" id="cb4-6" title="6">num_heads <span class="op">=</span> <span class="dv">12</span></a>
<a class="sourceLine" id="cb4-7" title="7">mlp_dim <span class="op">=</span> <span class="dv">3072</span></a>
<a class="sourceLine" id="cb4-8" title="8"></a>
<a class="sourceLine" id="cb4-9" title="9">vit_model <span class="op">=</span> VisionTransformer(</a>
<a class="sourceLine" id="cb4-10" title="10">    image_size<span class="op">=</span>image_size,</a>
<a class="sourceLine" id="cb4-11" title="11">    patch_size<span class="op">=</span>patch_size,</a>
<a class="sourceLine" id="cb4-12" title="12">    num_layers<span class="op">=</span>num_layers,</a>
<a class="sourceLine" id="cb4-13" title="13">    num_classes<span class="op">=</span>num_classes,</a>
<a class="sourceLine" id="cb4-14" title="14">    d_model<span class="op">=</span>d_model,</a>
<a class="sourceLine" id="cb4-15" title="15">    num_heads<span class="op">=</span>num_heads,</a>
<a class="sourceLine" id="cb4-16" title="16">    mlp_dim<span class="op">=</span>mlp_dim,</a>
<a class="sourceLine" id="cb4-17" title="17">)</a>
<a class="sourceLine" id="cb4-18" title="18"></a>
<a class="sourceLine" id="cb4-19" title="19">vit_model.<span class="bu">compile</span>(</a>
<a class="sourceLine" id="cb4-20" title="20">    optimizer<span class="op">=</span>tf.keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">1e-4</span>),</a>
<a class="sourceLine" id="cb4-21" title="21">    loss<span class="op">=</span>tf.keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</a>
<a class="sourceLine" id="cb4-22" title="22">    metrics<span class="op">=</span>[<span class="st">&quot;accuracy&quot;</span>],</a>
<a class="sourceLine" id="cb4-23" title="23">)</a></code></pre></div></li>
<li><p><strong>Train the Model</strong>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" title="1">vit_model.fit(train_dataset, epochs<span class="op">=</span>epochs, validation_data<span class="op">=</span>val_dataset)</a></code></pre></div></li>
</ol>
<p>This is a basic implementation of the Vision Transformer model. You can further improve the model by using techniques like data augmentation, learning rate scheduling, and fine-tuning with pre-trained weights.</p>
<hr />
<p><strong>Evaluation of Assistant A’s and Assistant B’s Answers:</strong></p>
<p><strong>Assistant A:</strong> - Provides a detailed step-by-step guide. - Includes code for defining the Vision Transformer architecture. - Covers dataset preparation, model creation, compilation, and training. - The response is comprehensive but quite lengthy.</p>
<p><strong>Assistant B:</strong> - Provides a more concise guide. - Includes steps for installing libraries, loading data, defining a simplified model, compiling, and training. - Uses a simpler model architecture that may not fully represent a Vision Transformer. - The response is easier to follow but lacks the depth and accuracy of a true Vision Transformer implementation.</p>
<p><strong>Final Verdict:</strong> Assistant A’s answer is more accurate and detailed, providing a true Vision Transformer implementation, while Assistant B’s answer is simpler but less accurate. Therefore, the final verdict is:</p>
<p><strong>Assistant A is significantly better: [[A&gt;&gt;B]]</strong></p>
</body>
</html>
