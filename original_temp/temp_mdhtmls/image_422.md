Hosting a large learning model on an average computer is possible, but it may require some adjustments and optimizations. Here are a few strategies that could help:

1. **Distributed computing**: Break down the model into smaller parts and distribute them across multiple computers. This can help reduce the computational load on each individual machine and improve overall performance.

2. **Optimized hardware**: Invest in hardware that is specifically designed for machine learning tasks. This could include GPUs, TPUs, or other specialized processors that can handle the computational demands of large models.

3. **Data compression**: Compress the model's weights and parameters to reduce the amount of memory required to store them. This can help free up space on the computer and improve performance.

4. **Pruning**: Remove unnecessary connections and neurons from the model to reduce its size and complexity. This can help improve performance and reduce the amount of memory required to run the model.

5. **Transfer learning**: Use a pre-trained model as a starting point and fine-tune it for your specific task. This can help reduce the amount of training data required and improve performance.

While it may be possible to host a large learning model on an average computer, it is unlikely that it would be able to achieve the same level of performance as paid options like GPT-3. GPT-3 is a state-of-the-art model that has been trained on a massive amount of data and has been optimized for performance. It is unlikely that an average computer could match the performance of GPT-3 without significant modifications and optimizations.