{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from text2img import text_to_image\n",
    "import concurrent.futures  \n",
    "def process_item(item, data_dir):  \n",
    "    question_id = item[\"question_id\"]  \n",
    "    answers = item['choices'][0]['turns'][0]['content']\n",
    "    model_id = item[\"model_id\"]\n",
    "    output_img = f\"image_{question_id}.png\"  \n",
    "    data_dir = \"data/images\"\n",
    "    text_to_image(answers, output_img, save_dir=model_id, data_dir=data_dir, temp_dir=\"original_temp\")  \n",
    "    # output_img = f\"output_{index}.png\"  \n",
    "    # text_to_image(item[\"gpt_answer\"], output_img, save_dir=\"gpt_response\", temp_dir=\"gpt_temp\")  \n",
    "    # output_img = f\"output_{index}.png\"  \n",
    "    # text_to_image(item[\"revised_text\"], output_img, save_dir=\"revised_response\", temp_dir=\"revised_temp\")  \n",
    "\n",
    "def generate_image(model_name, model_answers, data_dir):\n",
    "    max_workers = 20  \n",
    "    print(f\"-----------------generate {model_name} images -----------------\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:  \n",
    "        futures = [executor.submit(process_item, model_answers[model_name][key], data_dir) for key in model_answers[model_name].keys()]  \n",
    "        for future in concurrent.futures.as_completed(futures):  \n",
    "            try:  \n",
    "                future.result()  \n",
    "            except Exception as exc:  \n",
    "                print(f'Generated an exception: {exc}')  \n",
    "def generate_answer_images(model_answers, model_name, baseline_model, data_dir='data/images'):\n",
    "    if os.path.exists(os.path.join(data_dir, baseline_model)) == False:\n",
    "        generate_image(baseline_model, model_answers, data_dir)\n",
    "    \n",
    "    #generate images\n",
    "    generate_image(model_name, model_answers, data_dir)\n",
    "    \n",
    "    print(\"images generated done.\")\n",
    "# model_name = 'Meta-Llama-3.1-8B-Instruct'\n",
    "# baseline_model = 'gpt4_1106_preview'\n",
    "# generate_answer_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 09:27:26,292 - INFO - Starting the conversion process.\n",
      "2024-09-06 09:27:26,292 - INFO - Starting the conversion process.\n",
      "2024-09-06 09:27:26,293 - INFO - Starting the conversion process.\n",
      "2024-09-06 09:27:26,295 - INFO - Starting the conversion process.\n",
      "2024-09-06 09:27:26,296 - INFO - Starting the conversion process.\n",
      "2024-09-06 09:27:26,296 - INFO - Starting the conversion process.\n",
      "2024-09-06 09:27:26,298 - INFO - Starting the conversion process.\n",
      "2024-09-06 09:27:26,303 - INFO - Starting the conversion process.\n",
      "2024-09-06 09:27:26,305 - INFO - Starting the conversion process.\n",
      "2024-09-06 09:27:26,307 - INFO - Starting the conversion process.\n",
      "2024-09-06 09:27:26,308 - INFO - Starting the conversion process.\n",
      "2024-09-06 09:27:26,309 - INFO - Starting the conversion process.\n",
      "2024-09-06 09:27:26,311 - INFO - Starting the conversion process.\n",
      "2024-09-06 09:27:26,313 - INFO - Starting the conversion process.\n",
      "2024-09-06 09:27:26,314 - INFO - Starting the conversion process.\n",
      "[WAR2024-09-06 09:27:26,315 - INFO - Starting the conversion process.\n",
      "NING] This document format requires a nonempty <title> element.\n",
      "  Defaulting to 'image_0' as the title.\n",
      "  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n",
      "[WARNING] This document format requires a nonempty <title> element.\n",
      "  Defaulting to 'image_3' as the title.\n",
      "  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n",
      "[WARNING] This document format requires a nonempty <title> element.\n",
      "  Defaulting to 'image_2' as the title.\n",
      "  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n",
      "[WARNING] This document format requires a nonempty <title> element.\n",
      "  Defaulting to 'image_4' as the title.\n",
      "  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n",
      "[WARNING] This document format requires a nonempty <title> element.\n",
      "  Defaulting to 'image_6' as the title.\n",
      "  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n",
      "[WARNING] This document format requires a nonempty <title> element.\n",
      "  Defaulting to 'image_1' as the title.\n",
      "  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n",
      "[WARNING] This document format requires a nonempty <title> element.\n",
      "  Defaulting to 'image_5' as the title.\n",
      "  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n",
      "[WARNING] This document format requires a nonempty <title> element.\n",
      "  Defaulting to 'image_7' as the title.\n",
      "  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n",
      "[WARNING] This document format requires a nonempty <title> element.\n",
      "  Defaulting to 'image_8' as the title.\n",
      "  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n",
      "[WARNING] This document format requires a nonempty <title> element.\n",
      "  Defaulting to 'image_9' as the title.\n",
      "  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n",
      "[WARNING] This document format requires a nonempty <title> element.\n",
      "  Defaulting to 'image_12' as the title.\n",
      "  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n",
      "[[WWAARRNNII2024-09-06 09:27:26,315 - INFO - Starting the conversion process.\n",
      "NNGG]]  TThhiiss  ddooccuummeenntt  ffoorrmmaatt  rreeqquuiirreess  aa  nnoonneemmppttyy  <<ttiittllee>>  eelleemmeenntt..\n",
      "\n",
      "    DDeeffaauullttiinngg  ttoo  ''iimmaaggee__1110''  aass  tthhee  ttiittllee..\n",
      "\n",
      "    TToo  ssppeecciiffyy  aa  ttiittllee,,  uussee  ''ttiittllee''  iinn  mmeettaaddaattaa  oorr  ----mmeettaaddaattaa  ttiittllee==\"\"......\"\"..\n",
      "\n",
      "[WARNING] This document format requires a nonempty <title> element.\n",
      "  Defaulting to 'image_13' as the title.\n",
      "  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n",
      "2024-09-06 09:27:26,319 - INFO - Starting the conversion process.\n",
      "2024-09-06 09:27:26,321 - INFO - Starting the conversion process.\n",
      "2024-09-06 09:27:26,323 - INFO - Starting the conversion process.\n",
      "2024-09-06 09:27:26,370 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_5.html\n",
      "2024-09-06 09:27:26,370 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_2.html\n",
      "2024-09-06 09:27:26,371 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_0.html\n",
      "2024-09-06 09:27:26,371 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_7.html\n",
      "2024-09-06 09:27:26,371 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_4.html\n",
      "2024-09-06 09:27:26,371 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_6.html\n",
      "2024-09-06 09:27:26,371 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_1.html\n",
      "2024-09-06 09:27:26,371 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_3.html\n",
      "2024-09-06 09:27:26,372 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_8.html\n",
      "2024-09-06 09:27:26,373 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_9.html\n",
      "[2024-09-06 09:27:26,375 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_12.html\n",
      "WARNING] This document format requires a nonempty <title> element.\n",
      "  Defaulting to 'image_14' as the title.\n",
      "  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n",
      "[WARNING] This document format requires a nonempty <title> element.\n",
      "  Defaulting to 'image_16' as the title.\n",
      "  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n",
      "2024-09-06 09:27:26,376 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_10.html\n",
      "2024-09-06 09:27:26,378 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_11.html\n",
      "[WAR[NIWNAGR]N ING] TThhiiss  ddooccuummeenntt  ffoorrmmaatt  rreeqquuiirreess  aa  nnoonneemmppttyy  <<ttiittllee>>  eelleemmeenntt..\n",
      "\n",
      "    DDeeffaauullttiinngg  ttoo  ''iimmaaggee__1175''  aass  tthhee  ttiittllee..\n",
      "\n",
      "    TToo  ssppeecciiffyy  aa  ttiittllee,,  uussee  ''ttiittllee''  iinn  mmeettaaddaattaa  oorr  ----mmeettaaddaattaa  ttiittllee==\"\"......\"\"..\n",
      "\n",
      "[WARNING] This document format requires a nonempty <title> element.\n",
      "  Defaulting to 'image_18' as the title.\n",
      "  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n",
      "2024-09-06 09:27:26,385 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_13.html\n",
      "[WARNING] This document format requires a nonempty <title> element.\n",
      "  Defaulting to 'image_19' as the title.\n",
      "  To specify a title, use 'title' in metadata or --metadata title=\"...\".\n",
      "2024-09-06 09:27:26,420 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_14.html\n",
      "2024-09-06 09:27:26,432 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_16.html\n",
      "2024-09-06 09:27:26,434 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_15.html\n",
      "2024-09-06 09:27:26,437 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_17.html\n",
      "2024-09-06 09:27:26,439 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_18.html\n",
      "2024-09-06 09:27:26,439 - INFO - 转换成功: /home/v-lingjiang/project/arean_hard_eval/data/images/original_temp/image_19.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------generate Meta-Llama-3.1-8B-Instruct images -----------------\n"
     ]
    }
   ],
   "source": [
    "from utils import load_model_answers\n",
    "answer_dir = 'data/alpaca/model_answer'\n",
    "model_answers = load_model_answers(answer_dir)\n",
    "model_name = 'Meta-Llama-3.1-8B-Instruct'\n",
    "baseline_model = 'gpt4_1106_preview'\n",
    "generate_answer_images(model_answers, model_name, baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answers['Meta-Llama-3.1-8B-Instruct']['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answers['Meta-Llama-3.1-8B-Instruct']['0']['choices'][0]['turns'][0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "def get_alpaca_eval_data(dataset=\"alpaca_eval_gpt4_baseline\"):\n",
    "    dataset = load_dataset(\n",
    "        \"tatsu-lab/alpaca_eval\",\n",
    "        dataset,\n",
    "    )[\"eval\"]\n",
    "    return dataset\n",
    "alpaca_eval = get_alpaca_eval_data()\n",
    "def add_index(example, idx):  \n",
    "    example['index'] = idx  \n",
    "    return example  \n",
    "dataset = alpaca_eval.map(add_index, with_indices=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_eval[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数来添加索引  \n",
    "def add_index(example, idx):  \n",
    "    example['index'] = idx  \n",
    "    return example  \n",
    "dataset = alpaca_eval.map(add_index, with_indices=True) \n",
    "print(dataset[0])  # 打印第一行  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset  \n",
    "from transformers import AutoTokenizer  \n",
    "import hashlib  \n",
    "import time  \n",
    "\n",
    "from datasets import load_dataset\n",
    "def get_alpaca_eval_data(dataset=\"alpaca_eval_gpt4_baseline\"):\n",
    "    dataset = load_dataset(\n",
    "        \"tatsu-lab/alpaca_eval\",\n",
    "        dataset,\n",
    "    )[\"eval\"]\n",
    "    return dataset\n",
    "alpaca_eval = get_alpaca_eval_data()\n",
    "def add_index(example, idx):  \n",
    "    example['index'] = idx  \n",
    "    return example  \n",
    "dataset = alpaca_eval.map(add_index, with_indices=True) \n",
    "# 加载数据集  \n",
    "# dataset = load_dataset('alpaca_eval')  \n",
    "  \n",
    "# 定义一个函数来生成MD5哈希值  \n",
    "def generate_md5(text):  \n",
    "    return hashlib.md5(text.encode('utf-8')).hexdigest()  \n",
    "  \n",
    "# 使用预训练的分词器（例如，BERT的分词器）  \n",
    "tokenizer_path = \"/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/Meta-Llama-3.1-8B-Instruct\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)  \n",
    "  \n",
    "# 定义转换函数  \n",
    "def transform_example(example, idx):  \n",
    "    content = example['output']  \n",
    "    tokenized = tokenizer(content)  \n",
    "    token_len = len(tokenized['input_ids'])  \n",
    "      \n",
    "    transformed_example = {  \n",
    "        \"question_id\": str(idx),  \n",
    "        \"answer_id\": str(idx),  \n",
    "        \"model_id\": example['generator'],  \n",
    "        \"choices\": [  \n",
    "            {  \n",
    "                \"index\": 0,  \n",
    "                \"turns\": [  \n",
    "                    {  \n",
    "                        \"content\": content,  \n",
    "                        \"token_len\": token_len  \n",
    "                    }  \n",
    "                ]  \n",
    "            }  \n",
    "        ],  \n",
    "        # \"tstamp\": time.time()  \n",
    "    }  \n",
    "    return transformed_example  \n",
    "  \n",
    "# 对数据集进行转换  \n",
    "transformed_dataset = dataset.map(transform_example, with_indices=True, remove_columns=dataset.column_names)  \n",
    "  \n",
    "# 打印转换后的数据集的前几行  \n",
    "print(transformed_dataset[0])  \n",
    "transformed_dataset.to_json(\"gpt4_1106_preview.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset.to_json(\"temp_ref.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset  \n",
    "import hashlib  \n",
    "  \n",
    "# 加载数据集  \n",
    "def get_alpaca_eval_data(dataset=\"alpaca_eval_gpt4_baseline\"):  \n",
    "    dataset = load_dataset(\"tatsu-lab/alpaca_eval\", dataset)[\"eval\"]  \n",
    "    return dataset  \n",
    "  \n",
    "alpaca_eval = get_alpaca_eval_data()  \n",
    "  \n",
    "# 添加索引列  \n",
    "def add_index(example, idx):  \n",
    "    example['index'] = idx  \n",
    "    return example  \n",
    "  \n",
    "questions = alpaca_eval.map(add_index, with_indices=True)  \n",
    "  \n",
    "# 定义转换函数  \n",
    "def transform_example(example, idx):  \n",
    "    transformed_example = {  \n",
    "        \"question_id\": str(idx),  \n",
    "        \"category\": \"alpaca_eval\",  \n",
    "        \"cluster\": example['dataset'],  \n",
    "        \"turns\": [  \n",
    "            {  \n",
    "                \"content\": example['instruction']  \n",
    "            }  \n",
    "        ]  \n",
    "    }  \n",
    "    return transformed_example  \n",
    "  \n",
    "# 对数据集进行转换  \n",
    "transformed_questions = questions.map(transform_example, with_indices=True, remove_columns=dataset.column_names)  \n",
    "  \n",
    "# 打印转换后的数据集的前几行  \n",
    "print(transformed_questions[0])  \n",
    "transformed_questions.to_json(\"question.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "import datasets\n",
    "import json\n",
    "# template = \"{{ '<|begin_of_text|>' }}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + content + '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|eot_id|>' }}{% endif %}{% endfor %}\"\n",
    "template = \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message + '\\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ 'Human: ' + content + '\\nAssistant:' }}{% elif message['role'] == 'assistant' %}{{ content + '<|end_of_text|>' + '\\n' }}{% endif %}{% endfor %}\"\n",
    "\n",
    "model_name = \"Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name = \"Meta-Llama-3.1-8B\"\n",
    "\n",
    "# model_name = \"tulu_lora_sft_default_template_8b\"\n",
    "# model_name = \"tulu-2-dpo-7b\"\n",
    "# Create an LLM.\n",
    "# llm = LLM(model=f\"/home/lidong1/jianglingjie/LLama-Factory/model_checkpoint/huggingface/{model_name}\")\n",
    "\n",
    "llm = LLM(model=f\"/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints/{model_name}\")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "gen_kwargs_vllm = {\n",
    "    \"max_tokens\": 2048,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"temperature\": 0.0,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "}\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = template\n",
    "    # tokenizer.chat_template = tokenizer.chat_template.replace(\"<|eot_id|>\", tokenizer.eos_token)\n",
    "    # tokenizer.chat_template\n",
    "    gen_kwargs_vllm['stop_token_ids'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "    print(f\"tokenizer.chat_template: {tokenizer.chat_template}\")\n",
    "    print(\"tokenizer is None, use setted template\")\n",
    "else:\n",
    "    gen_kwargs_vllm['stop_token_ids'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\")]\n",
    "    print(\"use original template\")\n",
    "# messages = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(**gen_kwargs_vllm)\n",
    "\n",
    "eval_set = datasets.load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")[\"eval\"]\n",
    "eval_set = eval_set.select(range(10))\n",
    "def convert_to_message(example):  \n",
    "    messages = [{\"role\": \"user\", \"content\": example[\"instruction\"]}]  \n",
    "    example[\"messages\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  \n",
    "    return example  \n",
    "eval_set = eval_set.map(convert_to_message)\n",
    "# eval_set['messages']\n",
    "outputs = llm.generate(eval_set['messages'], sampling_params)\n",
    "print(outputs)\n",
    "outputs_text = [x.outputs[0].text for x in outputs]\n",
    "eval_set = eval_set.remove_columns([\"output\"])  # Remove the existing 'output' column if it exists  \n",
    "# eval_set = eval_set.remove_columns([\"messages\"])\n",
    "eval_set = eval_set.add_column(\"output\", outputs_text)  \n",
    "def rename_generator(sample):\n",
    "    sample['generator'] = f\"{model_name}\"\n",
    "    return sample\n",
    "eval_set = eval_set.map(rename_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0].metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "import datasets\n",
    "import json\n",
    "import argparse \n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Set model name.')  \n",
    "# parser.add_argument('--model-name', type=str, required=True, help='Name of the model to use')  \n",
    "# parser.add_argument('--model-path', type=str, default=\"/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints\", help='Dir of the model to use') \n",
    "# args = parser.parse_args()  \n",
    "path_dir = \"/mnt/lingjiejiang/textual_aesthetics/model_checkpoint/sft_merge_checkpoints\" \n",
    "model_name = \"Meta-Llama-3.1-8B-Instruct\" \n",
    "\n",
    "# template = \"{{ '<|begin_of_text|>' }}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + content + '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|eot_id|>' }}{% endif %}{% endfor %}\"\n",
    "template = \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message + '\\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ 'Human: ' + content + '\\nAssistant:' }}{% elif message['role'] == 'assistant' %}{{ content + '<|end_of_text|>' + '\\n' }}{% endif %}{% endfor %}\"\n",
    "\n",
    "# model_name = \"Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name = \"Meta-Llama-3.1-8B\"\n",
    "# Create an LLM.\n",
    "llm = LLM(model=f\"{path_dir}/{model_name}\")\n",
    "\n",
    "print(f\"model name: {model_name}\")\n",
    "print(f\"model_path: {path_dir}/{model_name}\")\n",
    "\n",
    "gen_kwargs_vllm = {\n",
    "    \"max_tokens\": 2048,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"temperature\": 0.0,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "}\n",
    "tokenizer = llm.get_tokenizer()\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = template\n",
    "    tokenizer.chat_template = tokenizer.chat_template.replace(\"<|eot_id|>\", tokenizer.eos_token)\n",
    "    # tokenizer.chat_template\n",
    "    gen_kwargs_vllm['stop_token_ids'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "    print(f\"tokenizer.chat_template: {tokenizer.chat_template}\")\n",
    "    print(\"tokenizer is None, use setted template\")\n",
    "else:\n",
    "    gen_kwargs_vllm['stop_token_ids'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\")]\n",
    "    print(\"use original template\")\n",
    "# messages = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(**gen_kwargs_vllm)\n",
    "\n",
    "# 加载question.jsonl数据集  \n",
    "questions = datasets.load_dataset('json', data_files='data/alpaca/question.jsonl')['train'] \n",
    "questions = questions.select(range(2)) \n",
    "print(questions[0])\n",
    "def convert_to_message(example):  \n",
    "    messages = [{\"role\": \"user\", \"content\": example[\"turns\"][0]['content']}]  \n",
    "    example[\"messages\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  \n",
    "    return example  \n",
    "questions = questions.map(convert_to_message)  \n",
    "\n",
    "# 生成输出  \n",
    "outputs = llm.generate(questions['messages'], sampling_params)  \n",
    "outputs_text = [x.outputs[0].text for x in outputs]  \n",
    "token_lens = [len(x.outputs[0].token_ids) for x in outputs]\n",
    "# 移除现有的 'output' 列并添加新的 'output' 列  \n",
    "questions = questions.remove_columns([\"messages\"])  \n",
    "questions = questions.add_column(\"output\", outputs_text)  \n",
    "questions = questions.add_column(\"token_lens\", token_lens) \n",
    "\n",
    "# 定义转换函数  \n",
    "def transform_example(example):  \n",
    "    content = example['output']  \n",
    "    # tokenized = tokenizer(content)  \n",
    "    token_len = example['token_lens']   \n",
    "      \n",
    "    transformed_example = {  \n",
    "        \"question_id\": example['question_id'],  \n",
    "        \"answer_id\": example['question_id'],  \n",
    "        \"model_id\": model_name,  \n",
    "        \"choices\": [  \n",
    "            {  \n",
    "                \"index\": 0,  \n",
    "                \"turns\": [  \n",
    "                    {  \n",
    "                        \"content\": content,  \n",
    "                        \"token_len\": token_len  \n",
    "                    }  \n",
    "                ]  \n",
    "            }  \n",
    "        ],  \n",
    "        # \"tstamp\": time.time()  \n",
    "    }  \n",
    "    return transformed_example  \n",
    "  \n",
    "# 对数据集进行转换  \n",
    "transformed_dataset = questions.map(transform_example, remove_columns=transform_example.column_names)  \n",
    "  \n",
    "# 保存为 JSONL 格式  \n",
    "with open(f'./data/alpaca/model_answer/{model_name}.jsonl', 'w', encoding='utf-8') as f:  \n",
    "    for example in transformed_dataset:  \n",
    "        json.dump(example, f)  \n",
    "        f.write('\\n')  \n",
    "  \n",
    "print(f\"Data saved to ./data/alpaca/model_answer/{model_name}.jsonl\")  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据集进行转换  \n",
    "transformed_dataset = questions.map(transform_example, remove_columns=questions.column_names)  \n",
    "  \n",
    "# 保存为 JSONL 格式  \n",
    "with open(f'./data/alpaca/model_answer/{model_name}.jsonl', 'w', encoding='utf-8') as f:  \n",
    "    for example in transformed_dataset:  \n",
    "        json.dump(example, f)  \n",
    "        f.write('\\n')  \n",
    "  \n",
    "print(f\"Data saved to ./data/alpaca/model_answer/{model_name}.jsonl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 生成输出  \n",
    "# outputs = llm.generate(questions['messages'], sampling_params)  \n",
    "# outputs_text = [x.outputs[0].text for x in outputs]  \n",
    "# token_lens = [len(x.outputs[0].token_ids) for x in outputs]\n",
    "# # 移除现有的 'output' 列并添加新的 'output' 列  \n",
    "# questions = questions.remove_columns([\"messages\"])  \n",
    "# questions = questions.add_column(\"output\", outputs_text)  \n",
    "# questions = questions.add_column(\"token_lens\", token_lens) \n",
    "\n",
    "# 定义转换函数  \n",
    "def transform_example(example):  \n",
    "    content = example['output']  \n",
    "    # tokenized = tokenizer(content)  \n",
    "    token_len = example['token_lens']   \n",
    "      \n",
    "    transformed_example = {  \n",
    "        \"question_id\": example['question_id'],  \n",
    "        \"answer_id\": example['question_id'],  \n",
    "        \"model_id\": model_name,  \n",
    "        \"choices\": [  \n",
    "            {  \n",
    "                \"index\": 0,  \n",
    "                \"turns\": [  \n",
    "                    {  \n",
    "                        \"content\": content,  \n",
    "                        \"token_len\": token_len  \n",
    "                    }  \n",
    "                ]  \n",
    "            }  \n",
    "        ],  \n",
    "        # \"tstamp\": time.time()  \n",
    "    }  \n",
    "    return transformed_example  \n",
    "  \n",
    "# 对数据集进行转换  \n",
    "transformed_dataset = questions.map(transform_example)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_message(example):  \n",
    "    messages = [{\"role\": \"user\", \"content\": example[\"turns\"][0]['content']}]  \n",
    "    example[\"messages\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  \n",
    "    return example  \n",
    "questions = questions.map(convert_to_message)  \n",
    "\n",
    "# 生成输出  \n",
    "outputs = llm.generate(questions['messages'], sampling_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams  \n",
    "from tqdm import tqdm  \n",
    "import datasets  \n",
    "import json  \n",
    "import argparse  \n",
    "import hashlib  \n",
    "import time  \n",
    "  \n",
    "# 解析命令行参数  \n",
    "parser = argparse.ArgumentParser(description='Set model name.')  \n",
    "parser.add_argument('--model-name', type=str, required=True, help='Name of the model to use')  \n",
    "parser.add_argument('--model-path', type=str, default=\"/home/lidong1/jianglingjie/LLama-Factory/model_checkpoint/huggingface\", help='Dir of the model to use')  \n",
    "args = parser.parse_args()  \n",
    "path_dir = args.model_path  \n",
    "model_name = args.model_name  \n",
    "  \n",
    "# 加载模型  \n",
    "llm = LLM(model=f\"{path_dir}/{model_name}\")  \n",
    "print(f\"model name: {model_name}\")  \n",
    "print(f\"model_path: {path_dir}/{model_name}\")  \n",
    "  \n",
    "# 定义生成参数  \n",
    "gen_kwargs_vllm = {  \n",
    "    \"max_tokens\": 2048,  \n",
    "    \"top_p\": 0.9,  \n",
    "    \"top_k\": 50,  \n",
    "    \"temperature\": 0.0,  \n",
    "    \"repetition_penalty\": 1.0,  \n",
    "}  \n",
    "  \n",
    "# 获取分词器  \n",
    "tokenizer = llm.get_tokenizer()  \n",
    "  \n",
    "# 设置模板  \n",
    "template = \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message + '\\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ 'Human: ' + content + '\\nAssistant:' }}{% elif message['role'] == 'assistant' %}{{ content + '<|end_of_text|>' + '\\n' }}{% endif %}{% endfor %}\"  \n",
    "  \n",
    "if tokenizer.chat_template is None:  \n",
    "    tokenizer.chat_template = template  \n",
    "    tokenizer.chat_template = tokenizer.chat_template.replace(\"<|eot_id|>\", tokenizer.eos_token)  \n",
    "    gen_kwargs_vllm['stop_token_ids'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]  \n",
    "    print(f\"tokenizer.chat_template: {tokenizer.chat_template}\")  \n",
    "    print(\"tokenizer is None, use setted template\")  \n",
    "else:  \n",
    "    gen_kwargs_vllm['stop_token_ids'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\")]  \n",
    "    print(\"use original template\")  \n",
    "  \n",
    "sampling_params = SamplingParams(**gen_kwargs_vllm)  \n",
    "  \n",
    "# 加载数据集  \n",
    "eval_set = datasets.load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")[\"eval\"]  \n",
    "  \n",
    "# 转换消息  \n",
    "def convert_to_message(example):  \n",
    "    messages = [{\"role\": \"user\", \"content\": example[\"instruction\"]}]  \n",
    "    example[\"messages\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  \n",
    "    return example  \n",
    "  \n",
    "eval_set = eval_set.map(convert_to_message)  \n",
    "  \n",
    "# 生成输出  \n",
    "outputs = llm.generate(eval_set['messages'], sampling_params)  \n",
    "outputs_text = [x.outputs[0].text for x in outputs]  \n",
    "  \n",
    "# 移除现有的 'output' 列并添加新的 'output' 列  \n",
    "eval_set = eval_set.remove_columns([\"output\"])  \n",
    "eval_set = eval_set.remove_columns([\"messages\"])  \n",
    "eval_set = eval_set.add_column(\"output\", outputs_text)  \n",
    "  \n",
    "# 重命名生成器  \n",
    "def rename_generator(sample):  \n",
    "    sample['generator'] = f\"{model_name}\"  \n",
    "    return sample  \n",
    "  \n",
    "eval_set = eval_set.map(rename_generator)  \n",
    "  \n",
    "# 定义转换函数  \n",
    "def transform_example(example, idx):  \n",
    "    content = example['output']  \n",
    "    tokenized = tokenizer(content)  \n",
    "    token_len = len(tokenized['input_ids'])  \n",
    "      \n",
    "    transformed_example = {  \n",
    "        \"question_id\": str(idx),  \n",
    "        \"answer_id\": str(idx),  \n",
    "        \"model_id\": example['generator'],  \n",
    "        \"choices\": [  \n",
    "            {  \n",
    "                \"index\": 0,  \n",
    "                \"turns\": [  \n",
    "                    {  \n",
    "                        \"content\": content,  \n",
    "                        \"token_len\": token_len  \n",
    "                    }  \n",
    "                ]  \n",
    "            }  \n",
    "        ],  \n",
    "        \"tstamp\": time.time()  \n",
    "    }  \n",
    "    return transformed_example  \n",
    "  \n",
    "# 对数据集进行转换  \n",
    "transformed_dataset = eval_set.map(transform_example, with_indices=True)  \n",
    "  \n",
    "# 保存为 JSONL 格式  \n",
    "with open(f'./data/{model_name}.jsonl', 'w', encoding='utf-8') as f:  \n",
    "    for example in transformed_dataset:  \n",
    "        json.dump(example, f)  \n",
    "        f.write('\\n')  \n",
    "  \n",
    "print(f\"Data saved to ./data/{model_name}.jsonl\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets  \n",
    "questions = datasets.load_dataset('json', data_files='data/alpaca/question.jsonl')['train']\n",
    "questions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams  \n",
    "from tqdm import tqdm  \n",
    "import datasets  \n",
    "import json  \n",
    "import argparse  \n",
    "import hashlib  \n",
    "import time  \n",
    "  \n",
    "# 解析命令行参数  \n",
    "parser = argparse.ArgumentParser(description='Set model name.')  \n",
    "parser.add_argument('--model-name', type=str, required=True, help='Name of the model to use')  \n",
    "parser.add_argument('--model-path', type=str, default=\"/home/lidong1/jianglingjie/LLama-Factory/model_checkpoint/huggingface\", help='Dir of the model to use')  \n",
    "args = parser.parse_args()  \n",
    "path_dir = args.model_path  \n",
    "model_name = args.model_name  \n",
    "  \n",
    "# 加载模型  \n",
    "llm = LLM(model=f\"{path_dir}/{model_name}\")  \n",
    "print(f\"model name: {model_name}\")  \n",
    "print(f\"model_path: {path_dir}/{model_name}\")  \n",
    "  \n",
    "# 定义生成参数  \n",
    "gen_kwargs_vllm = {  \n",
    "    \"max_tokens\": 2048,  \n",
    "    \"top_p\": 0.9,  \n",
    "    \"top_k\": 50,  \n",
    "    \"temperature\": 0.0,  \n",
    "    \"repetition_penalty\": 1.0,  \n",
    "}  \n",
    "  \n",
    "# 获取分词器  \n",
    "tokenizer = llm.get_tokenizer()  \n",
    "  \n",
    "# 设置模板  \n",
    "template = \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message + '\\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ 'Human: ' + content + '\\nAssistant:' }}{% elif message['role'] == 'assistant' %}{{ content + '<|end_of_text|>' + '\\n' }}{% endif %}{% endfor %}\"  \n",
    "  \n",
    "if tokenizer.chat_template is None:  \n",
    "    tokenizer.chat_template = template  \n",
    "    tokenizer.chat_template = tokenizer.chat_template.replace(\"<|eot_id|>\", tokenizer.eos_token)  \n",
    "    gen_kwargs_vllm['stop_token_ids'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]  \n",
    "    print(f\"tokenizer.chat_template: {tokenizer.chat_template}\")  \n",
    "    print(\"tokenizer is None, use setted template\")  \n",
    "else:  \n",
    "    gen_kwargs_vllm['stop_token_ids'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\")]  \n",
    "    print(\"use original template\")  \n",
    "  \n",
    "sampling_params = SamplingParams(**gen_kwargs_vllm)  \n",
    "  \n",
    "# 加载question.jsonl数据集  \n",
    "questions = datasets.load_dataset('json', data_files='data/alpaca/question.jsonl')['train']  \n",
    "  \n",
    "# 转换消息  \n",
    "def convert_to_message(example):  \n",
    "    messages = [{\"role\": \"user\", \"content\": example[\"turns\"][0][\"content\"]}]  \n",
    "    example[\"messages\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  \n",
    "    return example  \n",
    "  \n",
    "questions = questions.map(convert_to_message)  \n",
    "  \n",
    "# 生成输出  \n",
    "outputs = llm.generate(questions['messages'], sampling_params)  \n",
    "outputs_text = [x.outputs[0].text for x in outputs]  \n",
    "token_lens = [len(x.outputs[0].token_ids) for x in outputs]\n",
    "# 移除现有的 'output' 列并添加新的 'output' 列  \n",
    "questions = questions.remove_columns([\"messages\"])  \n",
    "questions = questions.add_column(\"output\", outputs_text)  \n",
    "questions = questions.add_column(\"token_lens\", token_lens) \n",
    "# 重命名生成器  \n",
    "def rename_generator(sample):  \n",
    "    sample['generator'] = f\"{model_name}\"  \n",
    "    return sample  \n",
    "  \n",
    "questions = questions.map(rename_generator)  \n",
    "  \n",
    "# 定义转换函数  \n",
    "def transform_example(example):  \n",
    "    content = example['output']  \n",
    "    # tokenized = tokenizer(content)  \n",
    "    token_len = example['token_lens']   \n",
    "      \n",
    "    transformed_example = {  \n",
    "        \"question_id\": example['question_id'],  \n",
    "        \"answer_id\": example['question_id'],  \n",
    "        \"model_id\": example['generator'],  \n",
    "        \"choices\": [  \n",
    "            {  \n",
    "                \"index\": 0,  \n",
    "                \"turns\": [  \n",
    "                    {  \n",
    "                        \"content\": content,  \n",
    "                        \"token_len\": token_len  \n",
    "                    }  \n",
    "                ]  \n",
    "            }  \n",
    "        ],  \n",
    "        \"tstamp\": time.time()  \n",
    "    }  \n",
    "    return transformed_example  \n",
    "  \n",
    "# 对数据集进行转换  \n",
    "transformed_dataset = questions.map(transform_example)  \n",
    "  \n",
    "# 保存为 JSONL 格式  \n",
    "with open(f'./data/{model_name}.jsonl', 'w', encoding='utf-8') as f:  \n",
    "    for example in transformed_dataset:  \n",
    "        json.dump(example, f)  \n",
    "        f.write('\\n')  \n",
    "  \n",
    "print(f\"Data saved to ./data/{model_name}.jsonl\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams  \n",
    "from tqdm import tqdm  \n",
    "import datasets  \n",
    "import json  \n",
    "import argparse  \n",
    "import hashlib  \n",
    "import time  \n",
    "  \n",
    "# 解析命令行参数  \n",
    "# parser = argparse.ArgumentParser(description='Set model name.')  \n",
    "# parser.add_argument('--model-name', type=str, required=True, help='Name of the model to use')  \n",
    "# parser.add_argument('--model-path', type=str, default=\"/home/lidong1/jianglingjie/LLama-Factory/model_checkpoint/huggingface\", help='Dir of the model to use')  \n",
    "# args = parser.parse_args()  \n",
    "path_dir = \"/home/lidong1/jianglingjie/LLama-Factory/model_checkpoint/huggingface\"\n",
    "model_name = args.model_name  \n",
    "  \n",
    "# 加载模型  \n",
    "llm = LLM(model=f\"{path_dir}/{model_name}\")  \n",
    "print(f\"model name: {model_name}\")  \n",
    "print(f\"model_path: {path_dir}/{model_name}\")  \n",
    "  \n",
    "# 定义生成参数  \n",
    "gen_kwargs_vllm = {  \n",
    "    \"max_tokens\": 2048,  \n",
    "    \"top_p\": 0.9,  \n",
    "    \"top_k\": 50,  \n",
    "    \"temperature\": 0.0,  \n",
    "    \"repetition_penalty\": 1.0,  \n",
    "}  \n",
    "  \n",
    "# 获取分词器  \n",
    "tokenizer = llm.get_tokenizer()  \n",
    "  \n",
    "# 设置模板  \n",
    "template = \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message + '\\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ 'Human: ' + content + '\\nAssistant:' }}{% elif message['role'] == 'assistant' %}{{ content + '<|end_of_text|>' + '\\n' }}{% endif %}{% endfor %}\"  \n",
    "  \n",
    "if tokenizer.chat_template is None:  \n",
    "    tokenizer.chat_template = template  \n",
    "    tokenizer.chat_template = tokenizer.chat_template.replace(\"<|eot_id|>\", tokenizer.eos_token)  \n",
    "    gen_kwargs_vllm['stop_token_ids'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]  \n",
    "    print(f\"tokenizer.chat_template: {tokenizer.chat_template}\")  \n",
    "    print(\"tokenizer is None, use setted template\")  \n",
    "else:  \n",
    "    gen_kwargs_vllm['stop_token_ids'] = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\")]  \n",
    "    print(\"use original template\")  \n",
    "  \n",
    "sampling_params = SamplingParams(**gen_kwargs_vllm)  \n",
    "  \n",
    "# 加载question.jsonl数据集  \n",
    "questions = datasets.load_dataset('json', data_files='data/alpaca/question.jsonl')['train']  \n",
    "questions = questions.select(range(10))\n",
    "# 转换消息  \n",
    "def convert_to_message(example):  \n",
    "    messages = [{\"role\": \"user\", \"content\": example[\"turns\"][0][\"content\"]}]  \n",
    "    example[\"messages\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  \n",
    "    return example  \n",
    "  \n",
    "questions = questions.map(convert_to_message)  \n",
    "  \n",
    "# 生成输出  \n",
    "outputs = llm.generate(questions['messages'], sampling_params)  \n",
    "outputs_text = [x.outputs[0].text for x in outputs]  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
